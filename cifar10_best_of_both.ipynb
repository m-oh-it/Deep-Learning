{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a64bfa34",
   "metadata": {},
   "source": [
    "\n",
    "# CIFAR-10 â€” Best-of-Both CNN (Merged & Improved)\n",
    "\n",
    "This notebook merges the clean modular design and optimizer experiments from **CIFAR10_CNN_Assignment.ipynb** with the stronger regularization and analysis from **assignment8.ipynb**, then **improves** the model with:\n",
    "- Keras **preprocessing/augmentation layers** (RandomFlip/Rotate/Zoom/Contrast)\n",
    "- **Conv â†’ BatchNorm â†’ SiLU** (swish) blocks + **Dropout**\n",
    "- **Label smoothing**, **AdamW** optimizer, **cosine decay** learning rate schedule\n",
    "- **EarlyStopping**, **ReduceLROnPlateau**, **ModelCheckpoint**\n",
    "- Clear **training curves**, **confusion matrix**, and **misclassified samples**\n",
    "\n",
    "> Expect ~75â€“80%+ test accuracy with reasonable training (varies by seed/hyperparams).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7864f04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# âœ… Setup\n",
    "import os, math, time, itertools, numpy as np, tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts\n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n",
    "np.random.seed(42); tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855c7bd7",
   "metadata": {},
   "source": [
    "## Load & Prepare CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1b7c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ“¦ Load\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "num_classes = 10\n",
    "\n",
    "# ðŸ§¹ Normalize to [0,1]\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test  = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# One-hot for analysis convenience (loss uses label smoothing with categorical targets)\n",
    "y_train_cat = to_categorical(y_train, num_classes)\n",
    "y_test_cat  = to_categorical(y_test,  num_classes)\n",
    "\n",
    "print(\"Train:\", x_train.shape, y_train.shape, \"| Test:\", x_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc2f5b1",
   "metadata": {},
   "source": [
    "## Data Augmentation (Keras Preprocessing Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b72a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.05),\n",
    "    layers.RandomZoom(0.1),\n",
    "    layers.RandomContrast(0.1),\n",
    "], name=\"augment\")\n",
    "\n",
    "# Optional: mixup/cutmix (advanced) â€“ left out for simplicity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b5c474",
   "metadata": {},
   "source": [
    "## Model â€” Conv/BN/SiLU Blocks + Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39868ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_block(x, filters, dropout=0.0):\n",
    "    x = layers.Conv2D(filters, 3, padding=\"same\", use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"swish\")(x)  # SiLU\n",
    "    if dropout > 0:\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "    return x\n",
    "\n",
    "def build_model(input_shape=(32,32,3), classes=10, base_width=64, dropout=0.25, dense_dropout=0.5):\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    x = data_augmentation(inp)\n",
    "\n",
    "    # Stem\n",
    "    x = conv_block(x, base_width, dropout=0.05)\n",
    "\n",
    "    # Blocks\n",
    "    for f in [base_width, base_width*2, base_width*2]:\n",
    "        x = conv_block(x, f, dropout=dropout)\n",
    "        x = conv_block(x, f, dropout=dropout)\n",
    "        x = layers.MaxPooling2D()(x)\n",
    "        x = layers.Dropout(0.1)(x)\n",
    "\n",
    "    x = layers.Conv2D(base_width*4, 1, padding=\"same\", use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"swish\")(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(dense_dropout)(x)\n",
    "    x = layers.Dense(256, activation=\"swish\")(x)\n",
    "    x = layers.Dropout(dense_dropout)(x)\n",
    "    out = layers.Dense(classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = models.Model(inp, out, name=\"BestOfBoth_CIFAR10\")\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355da342",
   "metadata": {},
   "source": [
    "## Optimizers & Compile Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589307a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_optimizer(opt_name=\"adamw\", initial_lr=3e-3, weight_decay=1e-4, steps_per_epoch=391, t0_epochs=10):\n",
    "    \"\"\"\n",
    "    opt_name: 'adamw' | 'adam' | 'rmsprop' | 'sgd'\n",
    "    Uses CosineDecayRestarts by default for smoother convergence.\n",
    "    \"\"\"\n",
    "    schedule = CosineDecayRestarts(initial_lr, first_decay_steps=max(steps_per_epoch*5, 200), t_mul=2.0, m_mul=0.8, alpha=0.0)\n",
    "\n",
    "    if opt_name.lower() == \"adamw\":\n",
    "        try:\n",
    "            return tf.keras.optimizers.AdamW(learning_rate=schedule, weight_decay=weight_decay)\n",
    "        except Exception:\n",
    "            # Fallback for older TF\n",
    "            return tf.keras.optimizers.Adam(learning_rate=schedule)\n",
    "    if opt_name.lower() == \"adam\":\n",
    "        return tf.keras.optimizers.Adam(learning_rate=schedule)\n",
    "    if opt_name.lower() == \"rmsprop\":\n",
    "        return tf.keras.optimizers.RMSprop(learning_rate=schedule, rho=0.9)\n",
    "    if opt_name.lower() == \"sgd\":\n",
    "        return tf.keras.optimizers.SGD(learning_rate=schedule, momentum=0.9, nesterov=True)\n",
    "    raise ValueError(\"Unknown optimizer: \" + opt_name)\n",
    "\n",
    "def compile_model(model, opt_name=\"adamw\", init_lr=3e-3):\n",
    "    opt = make_optimizer(opt_name=opt_name, initial_lr=init_lr)\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f3d971",
   "metadata": {},
   "source": [
    "## Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918cd4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_callbacks(run_name=\"best_of_both\"):\n",
    "    return [\n",
    "        callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=10, restore_best_weights=True, verbose=1),\n",
    "        callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=4, min_lr=1e-5, verbose=1),\n",
    "        callbacks.ModelCheckpoint(filepath=f\"{run_name}.keras\", monitor=\"val_accuracy\",\n",
    "                                  save_best_only=True, verbose=1),\n",
    "    ]\n",
    "\n",
    "def plot_history(history):\n",
    "    hist = history.history\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(hist[\"accuracy\"], label=\"train_acc\")\n",
    "    plt.plot(hist[\"val_accuracy\"], label=\"val_acc\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.title(\"Accuracy\")\n",
    "    plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(hist[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(hist[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Loss\")\n",
    "    plt.legend(); plt.grid(True); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b85168",
   "metadata": {},
   "source": [
    "## Fit a Strong Baseline (AdamW + Cosine LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be878789",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 40  # EarlyStopping will cut this if overfitting\n",
    "VAL_SPLIT = 0.1\n",
    "\n",
    "strategy = tf.distribute.get_strategy()\n",
    "with strategy.scope():\n",
    "    model = build_model(dense_dropout=0.5)  # keep stronger final dropout\n",
    "    model = compile_model(model, opt_name=\"adamw\", init_lr=3e-3)\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train_cat,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=VAL_SPLIT,\n",
    "    callbacks=get_callbacks(\"best_of_both_adamw\"),\n",
    "    verbose=2\n",
    ")\n",
    "plot_history(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed2fe60",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298537a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test_cat, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380156b6",
   "metadata": {},
   "source": [
    "## Confusion Matrix & Misclassified Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbee9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_names = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]\n",
    "\n",
    "y_pred = model.predict(x_test, verbose=0).argmax(axis=1)\n",
    "y_true = y_test.flatten()\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Classification report:\\n\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names, rotation=45)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show a few misclassified examples\n",
    "idxs = np.where(y_pred != y_true)[0][:25]\n",
    "plt.figure(figsize=(8,8))\n",
    "for i, idx in enumerate(idxs):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.imshow(x_test[idx])\n",
    "    plt.title(f\"T:{class_names[y_true[idx]]}\\nP:{class_names[y_pred[idx]]}\", fontsize=8)\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97529d65",
   "metadata": {},
   "source": [
    "## (Optional) Quick Optimizer Comparison (Short Runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5224c7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def quick_trial(opt_name, epochs=8, run_name=None):\n",
    "    with tf.distribute.get_strategy().scope():\n",
    "        m = build_model()\n",
    "        m = compile_model(m, opt_name=opt_name, init_lr=3e-3)\n",
    "    h = m.fit(\n",
    "        x_train, y_train_cat,\n",
    "        batch_size=128, epochs=epochs, validation_split=0.1, verbose=0\n",
    "    )\n",
    "    val_acc = max(h.history[\"val_accuracy\"])\n",
    "    return val_acc\n",
    "\n",
    "opts = [\"adamw\", \"adam\", \"rmsprop\", \"sgd\"]\n",
    "results = {opt: float(quick_trial(opt, epochs=6)) for opt in opts}\n",
    "print(\"Val accuracy (best across short runs):\")\n",
    "for k,v in results.items():\n",
    "    print(f\"{k:>7s}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c277638",
   "metadata": {},
   "source": [
    "## Export Model Definition to Python Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e03bcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "script_path = \"/mnt/data/cifar10_best_of_both.py\"\n",
    "with open(script_path, \"w\") as f:\n",
    "    f.write('''\n",
    "import numpy as np, tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def conv_block(x, filters, dropout=0.0):\n",
    "    x = layers.Conv2D(filters, 3, padding=\"same\", use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"swish\")(x)\n",
    "    if dropout > 0:\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "    return x\n",
    "\n",
    "def build_model(input_shape=(32,32,3), classes=10, base_width=64, dropout=0.25, dense_dropout=0.5):\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.05),\n",
    "        layers.RandomZoom(0.1),\n",
    "        layers.RandomContrast(0.1),\n",
    "    ])\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    x = data_augmentation(inp)\n",
    "    x = conv_block(x, base_width, dropout=0.05)\n",
    "    for f in [base_width, base_width*2, base_width*2]:\n",
    "        x = conv_block(x, f, dropout=dropout)\n",
    "        x = conv_block(x, f, dropout=dropout)\n",
    "        x = layers.MaxPooling2D()(x)\n",
    "        x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Conv2D(base_width*4, 1, padding=\"same\", use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"swish\")(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(dense_dropout)(x)\n",
    "    x = layers.Dense(256, activation=\"swish\")(x)\n",
    "    x = layers.Dropout(dense_dropout)(x)\n",
    "    out = layers.Dense(classes, activation=\"softmax\")(x)\n",
    "    return models.Model(inp, out, name=\"BestOfBoth_CIFAR10\")\n",
    "\n",
    "def make_optimizer(opt_name=\"adamw\", initial_lr=3e-3, weight_decay=1e-4, steps_per_epoch=391, t0_epochs=10):\n",
    "    schedule = CosineDecayRestarts(initial_lr, first_decay_steps=max(steps_per_epoch*5, 200), t_mul=2.0, m_mul=0.8, alpha=0.0)\n",
    "    if opt_name.lower() == \"adamw\":\n",
    "        try:\n",
    "            return tf.keras.optimizers.AdamW(learning_rate=schedule, weight_decay=weight_decay)\n",
    "        except Exception:\n",
    "            return tf.keras.optimizers.Adam(learning_rate=schedule)\n",
    "    if opt_name.lower() == \"adam\":\n",
    "        return tf.keras.optimizers.Adam(learning_rate=schedule)\n",
    "    if opt_name.lower() == \"rmsprop\":\n",
    "        return tf.keras.optimizers.RMSprop(learning_rate=schedule, rho=0.9)\n",
    "    if opt_name.lower() == \"sgd\":\n",
    "        return tf.keras.optimizers.SGD(learning_rate=schedule, momentum=0.9, nesterov=True)\n",
    "    raise ValueError(\"Unknown optimizer: \" + opt_name)\n",
    "\n",
    "def compile_model(model, opt_name=\"adamw\", init_lr=3e-3):\n",
    "    opt = make_optimizer(opt_name=opt_name, initial_lr=init_lr)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "''')\n",
    "print(\"Saved script to:\", script_path)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
